{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SRGAN_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvilmehta06/Image-Super-Resolution-Using-GAN-SRGAN/blob/main/SRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pscwnYeg7IAq"
      },
      "source": [
        "# SRGAN implementation in Pytorch\n",
        "\n",
        "Ref: [Code Link](https://github.com/leftthomas/SRGAN) \n",
        "\n",
        "Paper: [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/pdf/1609.04802.pdf)\n",
        "\n",
        "Dataset: \n",
        "* DIV2K - Valid\n",
        "  * [HR](https://data.vision.ee.ethz.ch/cvl/DIV2K/validation_release/DIV2K_valid_HR.zip)\n",
        "  * [LR](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X4.zip)\n",
        "* DIV2k - Train\n",
        "  * [HR](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip)\n",
        "  * [LR](https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X4.zip)\n",
        "\n",
        "Students: \n",
        "1. Ruchit Vithani (201701070)\n",
        "2. Purvil Mehta (201701073)\n",
        "3. Bhargey Mehta (201701074)\n",
        "4. Kushal Shah (201701111)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_175v1j742l"
      },
      "source": [
        "## Paths for saving weigths and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaoirkOgljd3"
      },
      "source": [
        "train_path = '/content/drive/MyDrive/DL_project_old/data/DIV2K_train_HR'\n",
        "val_path = '/content/drive/MyDrive/DL_project_old/data/DIV2K_valid_HR'\n",
        "\n",
        "G_weights_load = '/content/drive/MyDrive/DL_new/epochs/netG_epoch_4_100.pth'\n",
        "D_weights_load = '/content/drive/MyDrive/DL_new/epochs/netD_epoch_4_100.pth'\n",
        "\n",
        "imgs_save = '/content/drive/MyDrive/DL_new/training_results/sr/'\n",
        "\n",
        "G_weights_save = '/content/drive/MyDrive/DL_new/epochs/'\n",
        "D_weights_save = '/content/drive/MyDrive/DL_new/epochs/'\n",
        "\n",
        "out_stat_path = '/content/drive/MyDrive/DL_new/statistics/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfgbcZ3s7kpF"
      },
      "source": [
        "# Library Initialisation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxt665qElLj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e8a019-01bd-46a2-f2a7-4a3567e74dae"
      },
      "source": [
        "!pip install pytorch_ssim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_ssim\n",
            "  Downloading https://files.pythonhosted.org/packages/dc/78/f6cfa15ff7c66de5bb0873fb4bd699ff8024a0b00a94babbd216e64202b7/pytorch_ssim-0.1.tar.gz\n",
            "Building wheels for collected packages: pytorch-ssim\n",
            "  Building wheel for pytorch-ssim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-ssim: filename=pytorch_ssim-0.1-cp36-none-any.whl size=2027 sha256=ba25fedcbd8cbba9ae4f5b5924efc4ddc2e402ebc0ecb411515caa46b925b34e\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/60/c8/85a73ea90dcf1d39d5d7f94d83988511f0370229dee641bb79\n",
            "Successfully built pytorch-ssim\n",
            "Installing collected packages: pytorch-ssim\n",
            "Successfully installed pytorch-ssim-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZXUUF2ndZ64"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models.vgg import vgg16,vgg19\n",
        "from google.colab import files\n",
        "import math\n",
        "import torchvision\n",
        "import argparse\n",
        "from math import log10\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.utils as utils\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import tensorflow as tf\n",
        "import pytorch_ssim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hROfQYS5urxL",
        "outputId": "4995da63-c97f-4313-aeba-bd955289e8cd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ESWaAo77qsV"
      },
      "source": [
        "# Supporting Functions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZZzcyOl7vb6"
      },
      "source": [
        "## Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UllksoyVCOSJ"
      },
      "source": [
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
        "\n",
        "\n",
        "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
        "    return crop_size - (crop_size % upscale_factor)\n",
        "\n",
        "\n",
        "def train_hr_transform(crop_size):\n",
        "    return Compose([\n",
        "        RandomCrop(crop_size),\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def train_lr_transform(crop_size, upscale_factor):\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "def display_transform():\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(400),\n",
        "        CenterCrop(400),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "class TrainDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
        "        super(TrainDatasetFromFolder, self).__init__()\n",
        "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir)]\n",
        "        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
        "        self.hr_transform = train_hr_transform(crop_size)\n",
        "        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n",
        "        lr_image = self.lr_transform(hr_image)\n",
        "        return lr_image, hr_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "\n",
        "class ValDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, upscale_factor):\n",
        "        super(ValDatasetFromFolder, self).__init__()\n",
        "        self.upscale_factor = upscale_factor\n",
        "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir)]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hr_image = Image.open(self.image_filenames[index])\n",
        "        \n",
        "        w, h = hr_image.size\n",
        "        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n",
        "        lr_scale = Resize(crop_size // self.upscale_factor, interpolation=Image.BICUBIC)\n",
        "        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n",
        "        hr_image = CenterCrop(crop_size)(hr_image)\n",
        "        lr_image = lr_scale(hr_image)\n",
        "        hr_restore_img = hr_scale(lr_image)\n",
        "        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56peC5kK7x-O"
      },
      "source": [
        "## Loss Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uER6EykSKZrE"
      },
      "source": [
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        vgg = vgg19(pretrained=True)\n",
        "        loss_network = nn.Sequential(*list(vgg.features)[:34]).eval()\n",
        "        for param in loss_network.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.loss_network = loss_network\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        # Adversarial Loss\n",
        "        adversarial_loss = torch.mean(-torch.log(out_labels + 1e-6))\n",
        "        # Perception Loss\n",
        "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
        "        # Image Loss\n",
        "        image_loss = self.mse_loss(out_images, target_images)\n",
        "\n",
        "        return  image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4TCOZxN71KB"
      },
      "source": [
        "## Model Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHAtPgEcLLz1"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, scale_factor):\n",
        "        upsample_block_num = int(math.log(scale_factor, 2))\n",
        "\n",
        "        super(Generator, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.block2 = ResidualBlock(64)\n",
        "        self.block3 = ResidualBlock(64)\n",
        "        self.block4 = ResidualBlock(64)\n",
        "        self.block5 = ResidualBlock(64)\n",
        "        self.block6 = ResidualBlock(64)\n",
        "        self.block7 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n",
        "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
        "        self.block8 = nn.Sequential(*block8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(block1)\n",
        "        block3 = self.block3(block2)\n",
        "        block4 = self.block4(block3)\n",
        "        block5 = self.block5(block4)\n",
        "        block6 = self.block6(block5)\n",
        "        block7 = self.block7(block6)\n",
        "        block8 = self.block8(block1.clone() + block7)\n",
        "\n",
        "        return block8\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(512, 1024, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2, inplace=False),\n",
        "            nn.Conv2d(1024, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x1 = self.net(x)\n",
        "        x2 = x1.view(batch_size)\n",
        "        x3 = torch.sigmoid(x2)\n",
        "\n",
        "        return x3\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.prelu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        residual = self.bn2(residual)\n",
        "\n",
        "        return x.clone() + residual\n",
        "\n",
        "\n",
        "class UpsampleBLock(nn.Module):\n",
        "    def __init__(self, in_channels, up_scale):\n",
        "        super(UpsampleBLock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.prelu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwbBp66m7_W_"
      },
      "source": [
        "# Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQzoUQkvLwEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8634b1-251b-4909-dc1a-35a125377793"
      },
      "source": [
        "CROP_SIZE = 96\n",
        "UPSCALE_FACTOR = 4\n",
        "NUM_EPOCHS = 60\n",
        "\n",
        "train_set = TrainDatasetFromFolder(train_path, crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
        "val_set = ValDatasetFromFolder(val_path, upscale_factor=UPSCALE_FACTOR)\n",
        "train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "netG = Generator(UPSCALE_FACTOR)\n",
        "print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
        "netD = Discriminator()\n",
        "print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n",
        "\n",
        "# Incase, want to continue the training process from the previous weigths\n",
        "# netG.load_state_dict(torch.load(G_weights_load))\n",
        "# netD.load_state_dict(torch.load(D_weights_load))\n",
        "\n",
        "generator_criterion = GeneratorLoss()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    netG.cuda()\n",
        "    netD.cuda()\n",
        "    generator_criterion.cuda()\n",
        "\n",
        "optimizerG = optim.Adam(netG.parameters())\n",
        "optimizerD = optim.Adam(netD.parameters())\n",
        "\n",
        "results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': [], 'mse' : []}\n",
        "\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_bar = tqdm(train_loader)\n",
        "    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
        "\n",
        "    netG.train()\n",
        "    netD.train()\n",
        "\n",
        "    for data, target in train_bar:\n",
        "        g_update_first = True\n",
        "        batch_size = data.size(0)\n",
        "        running_results['batch_sizes'] += batch_size\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1-D(G(z)))\n",
        "        ###########################\n",
        "        real_img = torch.Tensor(target)\n",
        "        if torch.cuda.is_available():\n",
        "            real_img = real_img.cuda()\n",
        "        z = torch.Tensor(data)\n",
        "        if torch.cuda.is_available():\n",
        "            z = z.cuda()\n",
        "        fake_img = netG(z)\n",
        "\n",
        "        netD.zero_grad()\n",
        "        real_out_1 = netD(real_img)\n",
        "        real_out = torch.mean(real_out_1)\n",
        "        fake_out_1 = netD(fake_img)\n",
        "        fake_out = torch.mean(fake_out_1)\n",
        "        # if fake_out = 1, real_out = 0 => loss should be max \n",
        "        d_loss = -(torch.log(real_out + 1e-6) + torch.log(1-fake_out + 1e-6))\n",
        "        d_loss.backward(retain_graph=True)\n",
        "        \n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: minimize -log(D(G(z))) + Perception Loss + Image Loss\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
        "        g_loss.backward()\n",
        "        \n",
        "        fake_img = netG(z)\n",
        "        fake_out = netD(fake_img).mean()\n",
        "        \n",
        "        optimizerD.step()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # loss for current batch before optimization \n",
        "        running_results['g_loss'] += g_loss.item() * batch_size\n",
        "        running_results['d_loss'] += d_loss.item() * batch_size\n",
        "        running_results['d_score'] += real_out.item() * batch_size\n",
        "        running_results['g_score'] += fake_out.item() * batch_size\n",
        "\n",
        "        train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
        "            epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
        "            running_results['g_loss'] / running_results['batch_sizes'],\n",
        "            running_results['d_score'] / running_results['batch_sizes'],\n",
        "            running_results['g_score'] / running_results['batch_sizes']))\n",
        "    \n",
        "\n",
        "    netG.eval()\n",
        "    out_path = imgs_save\n",
        "    if not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_bar = tqdm(val_loader)\n",
        "        valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
        "        # val_images = []\n",
        "        for val_lr, val_hr_restore, val_hr in val_bar:\n",
        "            batch_size = val_lr.size(0)\n",
        "            valing_results['batch_sizes'] += batch_size\n",
        "            lr = val_lr\n",
        "            hr = val_hr\n",
        "            if torch.cuda.is_available():\n",
        "                lr = lr.cuda()\n",
        "                hr = hr.cuda()\n",
        "            sr = netG(lr)\n",
        "    \n",
        "            batch_mse = ((sr - hr) ** 2).data.mean()\n",
        "            valing_results['mse'] += batch_mse * batch_size\n",
        "            batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
        "            valing_results['ssims'] += batch_ssim * batch_size\n",
        "            valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
        "            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
        "            val_bar.set_description(\n",
        "                desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
        "                    valing_results['psnr'], valing_results['ssim']))\n",
        "\n",
        "            # val_images.extend(\n",
        "            #     [[lr.squeeze(0), hr.data.cpu().squeeze(0),\n",
        "            #         sr.data.cpu().squeeze(0)]])\n",
        "            \n",
        "\n",
        "        # val_save_bar = tqdm(val_images, desc='[saving training results]')\n",
        "        # index = 1\n",
        "        # for image in val_save_bar:\n",
        "        #     utils.save_image(image[0], out_path + \"lr_\" + str(index) + '.png')\n",
        "        #     utils.save_image(image[1], out_path + \"hr_\" + str(index) + '.png')\n",
        "        #     utils.save_image(image[2], out_path + \"sr_\" + str(index) + '.png')\n",
        "        #     index += 1\n",
        "    \n",
        "\n",
        "\n",
        "    # save loss\\scores\\psnr\\ssim\n",
        "    results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
        "    results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
        "    results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
        "    results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
        "    results['psnr'].append(valing_results['psnr'])\n",
        "    results['ssim'].append(valing_results['ssim'])\n",
        "    results['mse'].append(valing_results['mse'])\n",
        "    \n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        # save model parameters\n",
        "        torch.save(netG.state_dict(), G_weights_save + 'netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "        torch.save(netD.state_dict(), D_weights_save + 'netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "\n",
        "        data_frame = pd.DataFrame(\n",
        "            data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
        "                    'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim'], 'MSE' : results['mse']},\n",
        "            index=range(1, epoch + 1))\n",
        "        data_frame.to_csv(out_stat_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# generator parameters: 734219\n",
            "# discriminator parameters: 5215425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[1/60] Loss_D: 1.1772 Loss_G: 0.0339 D(x): 0.5724 D(G(z)): 0.4258: 100%|██████████| 50/50 [01:34<00:00,  1.89s/it]\n",
            "[converting LR images to SR images] PSNR: 19.0732 dB SSIM: 0.4964: 100%|██████████| 100/100 [00:36<00:00,  2.73it/s]\n",
            "[2/60] Loss_D: 1.2809 Loss_G: 0.0185 D(x): 0.5579 D(G(z)): 0.4775: 100%|██████████| 50/50 [01:23<00:00,  1.67s/it]\n",
            "[converting LR images to SR images] PSNR: 19.2237 dB SSIM: 0.4962: 100%|██████████| 100/100 [00:37<00:00,  2.70it/s]\n",
            "[3/60] Loss_D: 1.4001 Loss_G: 0.0165 D(x): 0.5232 D(G(z)): 0.5146: 100%|██████████| 50/50 [01:11<00:00,  1.42s/it]\n",
            "[converting LR images to SR images] PSNR: 17.7141 dB SSIM: 0.5328: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[4/60] Loss_D: 1.4289 Loss_G: 0.0142 D(x): 0.5156 D(G(z)): 0.5239: 100%|██████████| 50/50 [01:07<00:00,  1.35s/it]\n",
            "[converting LR images to SR images] PSNR: 19.1857 dB SSIM: 0.5639: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[5/60] Loss_D: 1.4430 Loss_G: 0.0138 D(x): 0.5965 D(G(z)): 0.5891: 100%|██████████| 50/50 [01:06<00:00,  1.33s/it]\n",
            "[converting LR images to SR images] PSNR: 19.4654 dB SSIM: 0.5677: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[6/60] Loss_D: 1.3885 Loss_G: 0.0115 D(x): 0.5400 D(G(z)): 0.5290: 100%|██████████| 50/50 [01:04<00:00,  1.30s/it]\n",
            "[converting LR images to SR images] PSNR: 21.1215 dB SSIM: 0.5705: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[7/60] Loss_D: 1.4035 Loss_G: 0.0105 D(x): 0.4795 D(G(z)): 0.4789: 100%|██████████| 50/50 [01:04<00:00,  1.29s/it]\n",
            "[converting LR images to SR images] PSNR: 22.1340 dB SSIM: 0.6280: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[8/60] Loss_D: 1.4277 Loss_G: 0.0106 D(x): 0.4804 D(G(z)): 0.4914: 100%|██████████| 50/50 [01:06<00:00,  1.34s/it]\n",
            "[converting LR images to SR images] PSNR: 22.4064 dB SSIM: 0.6407: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[9/60] Loss_D: 1.4142 Loss_G: 0.0086 D(x): 0.5054 D(G(z)): 0.5100: 100%|██████████| 50/50 [01:07<00:00,  1.35s/it]\n",
            "[converting LR images to SR images] PSNR: 22.0982 dB SSIM: 0.6237: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[10/60] Loss_D: 1.4135 Loss_G: 0.0092 D(x): 0.5607 D(G(z)): 0.5610: 100%|██████████| 50/50 [01:03<00:00,  1.28s/it]\n",
            "[converting LR images to SR images] PSNR: 22.5985 dB SSIM: 0.6504: 100%|██████████| 100/100 [00:37<00:00,  2.70it/s]\n",
            "[11/60] Loss_D: 1.4055 Loss_G: 0.0083 D(x): 0.5106 D(G(z)): 0.5102: 100%|██████████| 50/50 [01:03<00:00,  1.28s/it]\n",
            "[converting LR images to SR images] PSNR: 22.9910 dB SSIM: 0.6757: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[12/60] Loss_D: 1.3949 Loss_G: 0.0091 D(x): 0.5091 D(G(z)): 0.5040: 100%|██████████| 50/50 [01:03<00:00,  1.28s/it]\n",
            "[converting LR images to SR images] PSNR: 22.0070 dB SSIM: 0.6617: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[13/60] Loss_D: 1.3923 Loss_G: 0.0092 D(x): 0.5088 D(G(z)): 0.5075: 100%|██████████| 50/50 [01:04<00:00,  1.29s/it]\n",
            "[converting LR images to SR images] PSNR: 23.1680 dB SSIM: 0.6777: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[14/60] Loss_D: 1.3988 Loss_G: 0.0082 D(x): 0.4858 D(G(z)): 0.4869: 100%|██████████| 50/50 [01:04<00:00,  1.28s/it]\n",
            "[converting LR images to SR images] PSNR: 22.8211 dB SSIM: 0.6965: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[15/60] Loss_D: 1.3832 Loss_G: 0.0074 D(x): 0.5079 D(G(z)): 0.5034: 100%|██████████| 50/50 [01:04<00:00,  1.29s/it]\n",
            "[converting LR images to SR images] PSNR: 22.7305 dB SSIM: 0.6986: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n",
            "[16/60] Loss_D: 1.3939 Loss_G: 0.0078 D(x): 0.4996 D(G(z)): 0.4998: 100%|██████████| 50/50 [01:05<00:00,  1.30s/it]\n",
            "[converting LR images to SR images] PSNR: 23.4121 dB SSIM: 0.6996: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n",
            "[17/60] Loss_D: 1.3939 Loss_G: 0.0071 D(x): 0.5028 D(G(z)): 0.5029: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
            "[converting LR images to SR images] PSNR: 23.2972 dB SSIM: 0.6999: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[18/60] Loss_D: 1.3944 Loss_G: 0.0066 D(x): 0.4923 D(G(z)): 0.4926: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
            "[converting LR images to SR images] PSNR: 23.4554 dB SSIM: 0.7146: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[19/60] Loss_D: 1.3924 Loss_G: 0.0069 D(x): 0.5036 D(G(z)): 0.5041: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
            "[converting LR images to SR images] PSNR: 23.1811 dB SSIM: 0.6997: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[20/60] Loss_D: 1.3907 Loss_G: 0.0074 D(x): 0.4997 D(G(z)): 0.5002: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
            "[converting LR images to SR images] PSNR: 23.5492 dB SSIM: 0.7170: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n",
            "[21/60] Loss_D: 1.3932 Loss_G: 0.0073 D(x): 0.5008 D(G(z)): 0.5018: 100%|██████████| 50/50 [01:05<00:00,  1.30s/it]\n",
            "[converting LR images to SR images] PSNR: 22.5902 dB SSIM: 0.7016: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[22/60] Loss_D: 1.3855 Loss_G: 0.0071 D(x): 0.5020 D(G(z)): 0.5000: 100%|██████████| 50/50 [01:06<00:00,  1.33s/it]\n",
            "[converting LR images to SR images] PSNR: 23.8530 dB SSIM: 0.7268: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[23/60] Loss_D: 1.3866 Loss_G: 0.0067 D(x): 0.5042 D(G(z)): 0.5020: 100%|██████████| 50/50 [01:06<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 24.0529 dB SSIM: 0.7191: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n",
            "[24/60] Loss_D: 1.3865 Loss_G: 0.0062 D(x): 0.4966 D(G(z)): 0.4956: 100%|██████████| 50/50 [01:05<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 23.6607 dB SSIM: 0.7242: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[25/60] Loss_D: 1.3887 Loss_G: 0.0066 D(x): 0.5060 D(G(z)): 0.5051: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
            "[converting LR images to SR images] PSNR: 23.0991 dB SSIM: 0.7269: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[26/60] Loss_D: 1.3852 Loss_G: 0.0065 D(x): 0.4992 D(G(z)): 0.4974: 100%|██████████| 50/50 [01:05<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 23.6874 dB SSIM: 0.7286: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n",
            "[27/60] Loss_D: 1.3888 Loss_G: 0.0067 D(x): 0.4997 D(G(z)): 0.4998: 100%|██████████| 50/50 [01:06<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 22.5249 dB SSIM: 0.7082: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[28/60] Loss_D: 1.3850 Loss_G: 0.0064 D(x): 0.4979 D(G(z)): 0.4955: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
            "[converting LR images to SR images] PSNR: 22.7596 dB SSIM: 0.7226: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[29/60] Loss_D: 1.3858 Loss_G: 0.0063 D(x): 0.5056 D(G(z)): 0.5036: 100%|██████████| 50/50 [01:06<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 23.9010 dB SSIM: 0.7168: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[30/60] Loss_D: 1.3803 Loss_G: 0.0058 D(x): 0.4989 D(G(z)): 0.4929: 100%|██████████| 50/50 [01:06<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 24.4557 dB SSIM: 0.7349: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[31/60] Loss_D: 1.3726 Loss_G: 0.0059 D(x): 0.5062 D(G(z)): 0.4953: 100%|██████████| 50/50 [01:06<00:00,  1.33s/it]\n",
            "[converting LR images to SR images] PSNR: 24.2659 dB SSIM: 0.7217: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[32/60] Loss_D: 1.3896 Loss_G: 0.0062 D(x): 0.4956 D(G(z)): 0.4937: 100%|██████████| 50/50 [01:03<00:00,  1.28s/it]\n",
            "[converting LR images to SR images] PSNR: 23.8986 dB SSIM: 0.7332: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[33/60] Loss_D: 1.3824 Loss_G: 0.0061 D(x): 0.4997 D(G(z)): 0.4961: 100%|██████████| 50/50 [01:02<00:00,  1.26s/it]\n",
            "[converting LR images to SR images] PSNR: 23.6679 dB SSIM: 0.7292: 100%|██████████| 100/100 [00:37<00:00,  2.70it/s]\n",
            "[34/60] Loss_D: 1.3810 Loss_G: 0.0064 D(x): 0.5024 D(G(z)): 0.4966: 100%|██████████| 50/50 [01:02<00:00,  1.24s/it]\n",
            "[converting LR images to SR images] PSNR: 24.0781 dB SSIM: 0.7273: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[35/60] Loss_D: 1.3818 Loss_G: 0.0062 D(x): 0.5015 D(G(z)): 0.4965: 100%|██████████| 50/50 [01:01<00:00,  1.23s/it]\n",
            "[converting LR images to SR images] PSNR: 24.5193 dB SSIM: 0.7334: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[36/60] Loss_D: 1.3821 Loss_G: 0.0060 D(x): 0.4845 D(G(z)): 0.4730: 100%|██████████| 50/50 [01:01<00:00,  1.24s/it]\n",
            "[converting LR images to SR images] PSNR: 23.9553 dB SSIM: 0.7368: 100%|██████████| 100/100 [00:37<00:00,  2.70it/s]\n",
            "[37/60] Loss_D: 1.3983 Loss_G: 0.0058 D(x): 0.5219 D(G(z)): 0.5191: 100%|██████████| 50/50 [01:01<00:00,  1.23s/it]\n",
            "[converting LR images to SR images] PSNR: 24.1294 dB SSIM: 0.7339: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[38/60] Loss_D: 1.3939 Loss_G: 0.0056 D(x): 0.4953 D(G(z)): 0.4938: 100%|██████████| 50/50 [01:01<00:00,  1.23s/it]\n",
            "[converting LR images to SR images] PSNR: 24.5533 dB SSIM: 0.7421: 100%|██████████| 100/100 [00:37<00:00,  2.70it/s]\n",
            "[39/60] Loss_D: 1.3848 Loss_G: 0.0057 D(x): 0.4986 D(G(z)): 0.4942: 100%|██████████| 50/50 [01:01<00:00,  1.24s/it]\n",
            "[converting LR images to SR images] PSNR: 23.4397 dB SSIM: 0.7288: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[40/60] Loss_D: 1.3777 Loss_G: 0.0057 D(x): 0.4994 D(G(z)): 0.4875: 100%|██████████| 50/50 [01:01<00:00,  1.24s/it]\n",
            "[converting LR images to SR images] PSNR: 23.5214 dB SSIM: 0.7316: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[41/60] Loss_D: 1.3748 Loss_G: 0.0066 D(x): 0.5104 D(G(z)): 0.4969: 100%|██████████| 50/50 [01:01<00:00,  1.23s/it]\n",
            "[converting LR images to SR images] PSNR: 23.3468 dB SSIM: 0.7249: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[42/60] Loss_D: 1.3934 Loss_G: 0.0061 D(x): 0.4882 D(G(z)): 0.4841: 100%|██████████| 50/50 [01:01<00:00,  1.23s/it]\n",
            "[converting LR images to SR images] PSNR: 24.3579 dB SSIM: 0.7296: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[43/60] Loss_D: 1.3719 Loss_G: 0.0055 D(x): 0.5160 D(G(z)): 0.5039: 100%|██████████| 50/50 [01:02<00:00,  1.25s/it]\n",
            "[converting LR images to SR images] PSNR: 24.3880 dB SSIM: 0.7291: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[44/60] Loss_D: 1.3600 Loss_G: 0.0057 D(x): 0.5193 D(G(z)): 0.4997: 100%|██████████| 50/50 [01:03<00:00,  1.26s/it]\n",
            "[converting LR images to SR images] PSNR: 22.9974 dB SSIM: 0.7449: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[45/60] Loss_D: 1.3465 Loss_G: 0.0062 D(x): 0.4881 D(G(z)): 0.4549: 100%|██████████| 50/50 [01:03<00:00,  1.28s/it]\n",
            "[converting LR images to SR images] PSNR: 23.6689 dB SSIM: 0.7389: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[46/60] Loss_D: 1.4244 Loss_G: 0.0062 D(x): 0.5008 D(G(z)): 0.5040: 100%|██████████| 50/50 [01:03<00:00,  1.27s/it]\n",
            "[converting LR images to SR images] PSNR: 24.0849 dB SSIM: 0.7294: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[47/60] Loss_D: 1.4525 Loss_G: 0.0053 D(x): 0.6019 D(G(z)): 0.6035: 100%|██████████| 50/50 [01:04<00:00,  1.29s/it]\n",
            "[converting LR images to SR images] PSNR: 24.7101 dB SSIM: 0.7455: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[48/60] Loss_D: 1.4095 Loss_G: 0.0055 D(x): 0.4898 D(G(z)): 0.4924: 100%|██████████| 50/50 [01:03<00:00,  1.28s/it]\n",
            "[converting LR images to SR images] PSNR: 24.5899 dB SSIM: 0.7431: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[49/60] Loss_D: 1.4091 Loss_G: 0.0054 D(x): 0.5177 D(G(z)): 0.5203: 100%|██████████| 50/50 [01:04<00:00,  1.29s/it]\n",
            "[converting LR images to SR images] PSNR: 24.6009 dB SSIM: 0.7482: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[50/60] Loss_D: 1.4029 Loss_G: 0.0056 D(x): 0.5024 D(G(z)): 0.5027: 100%|██████████| 50/50 [01:04<00:00,  1.30s/it]\n",
            "[converting LR images to SR images] PSNR: 24.5720 dB SSIM: 0.7327: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[51/60] Loss_D: 1.4136 Loss_G: 0.0055 D(x): 0.5217 D(G(z)): 0.5228: 100%|██████████| 50/50 [01:04<00:00,  1.29s/it]\n",
            "[converting LR images to SR images] PSNR: 24.4972 dB SSIM: 0.7273: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[52/60] Loss_D: 1.3932 Loss_G: 0.0053 D(x): 0.5065 D(G(z)): 0.5059: 100%|██████████| 50/50 [01:05<00:00,  1.30s/it]\n",
            "[converting LR images to SR images] PSNR: 24.4941 dB SSIM: 0.7465: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
            "[53/60] Loss_D: 1.3975 Loss_G: 0.0054 D(x): 0.4990 D(G(z)): 0.4997: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
            "[converting LR images to SR images] PSNR: 24.1915 dB SSIM: 0.7382: 100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n",
            "[54/60] Loss_D: 1.3953 Loss_G: 0.0057 D(x): 0.5060 D(G(z)): 0.5059: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]\n",
            "[converting LR images to SR images] PSNR: 24.5599 dB SSIM: 0.7442: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n",
            "[55/60] Loss_D: 1.3874 Loss_G: 0.0056 D(x): 0.4997 D(G(z)): 0.4976: 100%|██████████| 50/50 [01:05<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 24.8460 dB SSIM: 0.7441: 100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n",
            "[56/60] Loss_D: 1.3875 Loss_G: 0.0056 D(x): 0.4963 D(G(z)): 0.4944: 100%|██████████| 50/50 [01:06<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 24.9280 dB SSIM: 0.7466: 100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n",
            "[57/60] Loss_D: 1.3846 Loss_G: 0.0054 D(x): 0.5054 D(G(z)): 0.5019: 100%|██████████| 50/50 [01:06<00:00,  1.33s/it]\n",
            "[converting LR images to SR images] PSNR: 24.8081 dB SSIM: 0.7479: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n",
            "[58/60] Loss_D: 1.3765 Loss_G: 0.0053 D(x): 0.5060 D(G(z)): 0.4986: 100%|██████████| 50/50 [01:06<00:00,  1.34s/it]\n",
            "[converting LR images to SR images] PSNR: 23.9885 dB SSIM: 0.7466: 100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n",
            "[59/60] Loss_D: 1.3651 Loss_G: 0.0059 D(x): 0.5111 D(G(z)): 0.4968: 100%|██████████| 50/50 [01:05<00:00,  1.32s/it]\n",
            "[converting LR images to SR images] PSNR: 23.6709 dB SSIM: 0.7357: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
            "[60/60] Loss_D: 1.3352 Loss_G: 0.0055 D(x): 0.4987 D(G(z)): 0.4657: 100%|██████████| 50/50 [01:04<00:00,  1.30s/it]\n",
            "[converting LR images to SR images] PSNR: 23.6203 dB SSIM: 0.7457: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1GIxsJE_YFU"
      },
      "source": [
        "# Validation - Generate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkHYtTJNYUO-"
      },
      "source": [
        "def buildG(UPSCALE_FACTOR=4):\n",
        "    netG = Generator(UPSCALE_FACTOR)\n",
        "    netG.train()\n",
        "    netG.load_state_dict(torch.load(G_weights_load))\n",
        "    netG.cuda()\n",
        "\n",
        "    return netG\n",
        "\n",
        "netG = buildG()\n",
        "def test_on_single_image(path,UPSCALE_FACTOR=4, index=0):\n",
        "    img = Image.open(path)\n",
        "    width, heigth = img.size\n",
        "    img = img.resize((width//4,heigth//4),Image.BILINEAR)\n",
        "    layer = ToTensor()\n",
        "    img3 = layer(img)\n",
        "    sh = img3.shape\n",
        "    img3 =img3.reshape((1, sh[0], sh[1], sh[2]))\n",
        "    img2 = netG(img3.cuda(0))\n",
        "    img_path = imgs_save +'sr-final_'+ str(index) + '.png'\n",
        "    torchvision.utils.save_image(img2, img_path)\n",
        "    #files.download(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bPZGMUc5ACW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf56d97c-8e94-4a67-f834-5c6fa2e97421"
      },
      "source": [
        "dataset_dir = '/content/drive/MyDrive/test_images/hr'\n",
        "image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir)]\n",
        "i=1\n",
        "for path in image_filenames:\n",
        "    print(path)\n",
        "    test_on_single_image(path, 4, i)\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/test_images/hr/hr1.png\n",
            "/content/drive/MyDrive/test_images/hr/hr2.png\n",
            "/content/drive/MyDrive/test_images/hr/hr3.png\n",
            "/content/drive/MyDrive/test_images/hr/hr4.png\n",
            "/content/drive/MyDrive/test_images/hr/hr5.png\n",
            "/content/drive/MyDrive/test_images/hr/hr6.png\n",
            "/content/drive/MyDrive/test_images/hr/hr7.png\n",
            "/content/drive/MyDrive/test_images/hr/hr8.png\n",
            "/content/drive/MyDrive/test_images/hr/hr9.png\n",
            "/content/drive/MyDrive/test_images/hr/hr10.png\n",
            "/content/drive/MyDrive/test_images/hr/hr11.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRZxyxqw_Rea"
      },
      "source": [
        "# Validation On 100 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT__7vyNehSf"
      },
      "source": [
        "# import argparse\n",
        "# import os\n",
        "# from math import log10\n",
        "\n",
        "# import pandas as pd\n",
        "# import torch.optim as optim\n",
        "# import torch.utils.data\n",
        "# import torchvision.utils as utils\n",
        "# from torch.autograd import Variable\n",
        "# from torch.utils.data import DataLoader\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# import pytorch_ssim\n",
        "\n",
        "# CROP_SIZE = 96\n",
        "# UPSCALE_FACTOR = 4\n",
        "# NUM_EPOCHS = 100\n",
        "\n",
        "# train_set = TrainDatasetFromFolder('/content/DIV2K_train_HR', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
        "# val_set = ValDatasetFromFolder('/content/DIV2K_valid_HR', upscale_factor=UPSCALE_FACTOR)\n",
        "# train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=16, shuffle=True)\n",
        "# val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)\n",
        "\n",
        "# netG = Generator(UPSCALE_FACTOR)\n",
        "# print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
        "# netD = Discriminator()\n",
        "# print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n",
        "\n",
        "# netG.load_state_dict(torch.load('/content/drive/MyDrive/DL_project (1)/epochs/netG_epoch_4_99.pth'))\n",
        "# netD.load_state_dict(torch.load('/content/drive/MyDrive/DL_project (1)/epochs/netD_epoch_4_99.pth'))\n",
        "\n",
        "# generator_criterion = GeneratorLoss()\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     netG.cuda()\n",
        "#     netD.cuda()\n",
        "#     generator_criterion.cuda()\n",
        "\n",
        "# optimizerG = optim.Adam(netG.parameters())\n",
        "# optimizerD = optim.Adam(netD.parameters())\n",
        "\n",
        "# results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
        "\n",
        "# # torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# for epoch in range(101, NUM_EPOCHS + 101):\n",
        "#     train_bar = tqdm(train_loader)\n",
        "#     running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
        "\n",
        "#     netG.train()\n",
        "#     netD.train()\n",
        "\n",
        "#     for data, target in train_bar:\n",
        "#         g_update_first = True\n",
        "#         batch_size = data.size(0)\n",
        "#         running_results['batch_sizes'] += batch_size\n",
        "\n",
        "#         ############################\n",
        "#         # (1) Update D network: maximize D(x)-1-D(G(z))\n",
        "#         ###########################\n",
        "#         real_img = torch.Tensor(target)\n",
        "#         if torch.cuda.is_available():\n",
        "#             real_img = real_img.cuda()\n",
        "#         z = torch.Tensor(data)\n",
        "#         if torch.cuda.is_available():\n",
        "#             z = z.cuda()\n",
        "#         fake_img = netG(z)\n",
        "\n",
        "#         netD.zero_grad()\n",
        "#         real_out_1 = netD(real_img)\n",
        "#         real_out = torch.mean(real_out_1)\n",
        "#         fake_out_1 = netD(fake_img)\n",
        "#         fake_out = torch.mean(fake_out_1)\n",
        "#         d_loss = 1 - real_out + fake_out\n",
        "#         d_loss.backward(retain_graph=True)\n",
        "        \n",
        "\n",
        "#         ############################\n",
        "#         # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss\n",
        "#         ###########################\n",
        "#         netG.zero_grad()\n",
        "#         g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
        "#         g_loss.backward()\n",
        "        \n",
        "#         fake_img = netG(z)\n",
        "#         fake_out = netD(fake_img).mean()\n",
        "        \n",
        "#         optimizerD.step()\n",
        "#         optimizerG.step()\n",
        "\n",
        "#         # loss for current batch before optimization \n",
        "#         running_results['g_loss'] += g_loss.item() * batch_size\n",
        "#         running_results['d_loss'] += d_loss.item() * batch_size\n",
        "#         running_results['d_score'] += real_out.item() * batch_size\n",
        "#         running_results['g_score'] += fake_out.item() * batch_size\n",
        "\n",
        "#         train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
        "#             epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
        "#             running_results['g_loss'] / running_results['batch_sizes'],\n",
        "#             running_results['d_score'] / running_results['batch_sizes'],\n",
        "#             running_results['g_score'] / running_results['batch_sizes']))\n",
        "         \n",
        "\n",
        "#     netG.eval()\n",
        "#     out_path = '/content/drive/MyDrive/DL_project (1)/training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
        "#     if not os.path.exists(out_path):\n",
        "#         os.makedirs(out_path)\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#         val_bar = tqdm(val_loader)\n",
        "#         valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
        "#         val_images = []\n",
        "#         for val_lr, val_hr_restore, val_hr in val_bar:\n",
        "#             batch_size = val_lr.size(0)\n",
        "#             valing_results['batch_sizes'] += batch_size\n",
        "#             lr = val_lr\n",
        "#             hr = val_hr\n",
        "#             if torch.cuda.is_available():\n",
        "#                 lr = lr.cuda()\n",
        "#                 hr = hr.cuda()\n",
        "#             sr = netG(lr)\n",
        "    \n",
        "#             batch_mse = ((sr - hr) ** 2).data.mean()\n",
        "#             valing_results['mse'] += batch_mse * batch_size\n",
        "#             batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
        "#             valing_results['ssims'] += batch_ssim * batch_size\n",
        "#             valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
        "#             valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
        "#             val_bar.set_description(\n",
        "#                 desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
        "#                     valing_results['psnr'], valing_results['ssim']))\n",
        "            \n",
        "#             val_images.extend(\n",
        "#                 [display_transform()(lr.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
        "#                     display_transform()(sr.data.cpu().squeeze(0))])\n",
        "            \n",
        "\n",
        "#         val_images = torch.stack(val_images)\n",
        "#         val_images = torch.chunk(val_images, val_images.size(0) // 15)\n",
        "#         val_save_bar = tqdm(val_images, desc='[saving training results]')\n",
        "#         index = 1\n",
        "#         for image in val_save_bar:\n",
        "#             image = utils.make_grid(image, nrow=3, padding=5)\n",
        "#             utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
        "#             index += 1\n",
        "\n",
        "#     # save model parameters\n",
        "#     torch.save(netG.state_dict(), '/content/drive/MyDrive/DL_project (1)/epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "#     torch.save(netD.state_dict(), '/content/drive/MyDrive/DL_project (1)/epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "#     # save loss\\scores\\psnr\\ssim\n",
        "#     results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
        "#     results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
        "#     results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
        "#     results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
        "#     results['psnr'].append(valing_results['psnr'])\n",
        "#     results['ssim'].append(valing_results['ssim'])\n",
        "\n",
        "#     if epoch % 10 == 0 and epoch != 0:\n",
        "#         out_path = '/content/drive/MyDrive/DL_project (1)/statistics/'\n",
        "#         data_frame = pd.DataFrame(\n",
        "#             data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
        "#                     'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
        "#             index=range(1, epoch + 1))\n",
        "#         data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}